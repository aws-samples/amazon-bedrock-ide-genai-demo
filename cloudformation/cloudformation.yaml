AWSTemplateFormatVersion: "2010-09-09"
Description: AWS CloudFormation template to set up a single API Gateway for accessing Sales dataset.

Resources:
  SalesDataS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "${AWS::StackName}-sales-data-${AWS::AccountId}"

  SchemaInstructionS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "${AWS::StackName}-agent-instructions-${AWS::AccountId}"

  SalesDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: sales
        Description: Database for Sales dataset

  DownloadSalesDataLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3DataBucketAccessPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:ListBucket
                  - s3:GetObject
                Resource: !Sub "arn:aws:s3:::${SalesDataS3Bucket}/sales-data/*"

  DownloadSalesDataLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt DownloadSalesDataLambdaExecutionRole.Arn
      EphemeralStorage:
        Size: 2048
      Code:
        ZipFile: |
          import boto3
          import urllib.request
          import urllib.error
          import os
          import cfnresponse
          import zipfile
          import csv
          import io

          def lambda_handler(event, context):
              try:
                  if event['RequestType'] == 'Create':
                      s3 = boto3.client('s3')
                      bucket = os.environ['SalesDataS3Bucket']
                      prefix = "sales-data/"

                      # Download the zip file with headers
                      url = "https://excelbianalytics.com/wp/wp-content/uploads/2017/07/1000-Sales-Records.zip"
                      headers = {
                          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                          'Accept-Language': 'en-US,en;q=0.5',
                          'Connection': 'keep-alive',
                          'Upgrade-Insecure-Requests': '1'
                      }
                      
                      try:
                          req = urllib.request.Request(url, headers=headers)
                          with urllib.request.urlopen(req) as response:
                              with open('/tmp/sales_data.zip', 'wb') as out_file:
                                  out_file.write(response.read())
                      except urllib.error.HTTPError as e:
                          print(f"HTTP Error: {e.code} - {e.reason}")
                          print(f"Error response headers: {e.headers}")
                          raise
                      except urllib.error.URLError as e:
                          print(f"URL Error: {e.reason}")
                          raise
                      except Exception as e:
                          print(f"Unexpected error during download: {str(e)}")
                          raise

                      try:
                          # Extract and process the CSV
                          with zipfile.ZipFile('/tmp/sales_data.zip', 'r') as zip_ref:
                              file_list = zip_ref.namelist()
                              print(f"Files in zip: {file_list}")  # Debug info
                              csv_filename = next((f for f in file_list if f.endswith('.csv')), None)
                              if not csv_filename:
                                  raise ValueError("No CSV file found in the zip archive")
                              zip_ref.extract(csv_filename, '/tmp/')
                              
                              # Upload to S3
                              print(f"Uploading {csv_filename} to S3")
                              s3.upload_file(f'/tmp/{csv_filename}', bucket, prefix + 'sales_records.csv')
                      except Exception as e:
                          print(f"Error processing zip file: {str(e)}")
                          raise

                      responseData = {'Message': 'Sales dataset downloaded and uploaded to S3'}
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
                  elif event['RequestType'] == 'Delete':
                      responseData = {'Message': 'Custom resource deleted'}
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
              except Exception as e:
                  print(f'Error in lambda_handler: {str(e)}')
                  cfnresponse.send(event, context, cfnresponse.FAILED, {
                      "Error": str(e)
                  })

      Runtime: python3.8
      Timeout: 300
      Environment:
        Variables:
          SalesDataS3Bucket: !Ref SalesDataS3Bucket
      MemorySize: 512

  DownloadSalesDataCustomResource:
    Type: Custom::DownloadSalesData
    Properties:
      ServiceToken: !GetAtt DownloadSalesDataLambdaFunction.Arn

  SalesRecordsTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: sales
      TableInput:
        Name: sales_records
        TableType: EXTERNAL_TABLE
        Parameters:
          "skip.header.line.count": "1"
          "classification": "csv"
          "delimiter": ","
        StorageDescriptor:
          Columns:
            - Name: region
              Type: string
            - Name: country
              Type: string
            - Name: item_type
              Type: string
            - Name: sales_channel
              Type: string
            - Name: order_priority
              Type: string
            - Name: order_date
              Type: string
            - Name: order_id
              Type: string
            - Name: ship_date
              Type: string
            - Name: units_sold
              Type: int
            - Name: unit_price
              Type: double
            - Name: unit_cost
              Type: double
            - Name: total_revenue
              Type: double
            - Name: total_cost
              Type: double
            - Name: total_profit
              Type: double
          Location: !Sub "s3://${SalesDataS3Bucket}/sales-data/"
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Parameters:
              field.delim: ","
              serialization.format: ","

  AthenaQueryPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - athena:StartQueryExecution
              - athena:GetQueryExecution
              - athena:GetQueryResults
              - athena:StopQueryExecution
              - athena:ListQueryExecutions
              - athena:BatchGetQueryExecution
              - athena:ListDatabases
            Resource: !Sub 'arn:aws:athena:${AWS::Region}:${AWS::AccountId}:workgroup/primary'
          - Effect: Allow
            Action:
              - athena:GetWorkGroup
              - athena:ListWorkGroups
              - athena:ListDatabases
              - athena:GetDatabase
            Resource: '*'
          - Effect: Allow
            Action:
              - glue:GetDatabases
              - glue:GetTables
              - glue:GetTable
              - glue:GetPartitions
              - glue:GetDatabase
              - glue:CreateTable
              - glue:UpdateTable
              - glue:DeleteTable
              - glue:BatchDeleteTable
            Resource:
              - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog'
              - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/*'
              - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/*'
          - Effect: Allow
            Action:
              - s3:GetBucketLocation
              - s3:GetObject
              - s3:ListBucket
              - s3:ListBucketMultipartUploads
              - s3:ListMultipartUploadParts
              - s3:AbortMultipartUpload
              - s3:PutObject
              - s3:DeleteObject
            Resource:
              - !Sub 'arn:aws:s3:::${SalesDataS3Bucket}'
              - !Sub 'arn:aws:s3:::${SalesDataS3Bucket}/*'
              - !Sub 'arn:aws:s3:::aws-athena-query-results-${AWS::AccountId}-${AWS::Region}'
              - !Sub 'arn:aws:s3:::aws-athena-query-results-${AWS::AccountId}-${AWS::Region}/*'

  QueryAthenaLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - !Ref AthenaQueryPolicy

  PopulateParquetTablesLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt QueryAthenaLambdaExecutionRole.Arn
      Code:
        ZipFile: |
          import boto3
          import time
          import cfnresponse
          import logging

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          def lambda_handler(event, context):
              logger.info(f"Received event: {event}")
              
              try:
                  if event['RequestType'] == 'Delete':
                      handle_delete(event, context)
                  elif event['RequestType'] in ['Create', 'Update']:
                      handle_create_update(event, context)
                  else:
                      raise ValueError(f"Unsupported request type: {event['RequestType']}")
              except Exception as e:
                  logger.error(f"Error: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": str(e)})

          def handle_delete(event, context):
              logger.info("Handling DELETE event")
              
              athena_client = boto3.client('athena')
              s3_client = boto3.client('s3')
              s3_bucket = event['ResourceProperties']['OutputBucket']
              s3_output = f"s3://{s3_bucket}/athena_query_results/"

              try:
                  query = "DROP TABLE IF EXISTS sales_records_parquet"
                  run_query(athena_client, query, s3_output)
                  logger.info("Parquet table dropped successfully")

                  # Empty S3 bucket
                  empty_s3_bucket(s3_client, s3_bucket)
                  logger.info(f"S3 bucket {s3_bucket} emptied successfully")
              except Exception as e:
                  logger.warning(f"Error during cleanup: {str(e)}")
              
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})

          def empty_s3_bucket(s3_client, bucket_name):
              paginator = s3_client.get_paginator('list_objects_v2')
              delete_us = dict(Objects=[])
              for page in paginator.paginate(Bucket=bucket_name):
                  for obj in page.get('Contents', []):
                      delete_us['Objects'].append(dict(Key=obj['Key']))
                      if len(delete_us['Objects']) >= 1000:
                          s3_client.delete_objects(Bucket=bucket_name, Delete=delete_us)
                          delete_us = dict(Objects=[])
              
              if len(delete_us['Objects']):
                  s3_client.delete_objects(Bucket=bucket_name, Delete=delete_us)

          def handle_create_update(event, context):
              athena_client = boto3.client('athena')
              s3_bucket = event['ResourceProperties']['OutputBucket']
              s3_output = f"s3://{s3_bucket}/athena_query_results/"

              try:
                  # Check if database exists
                  databases = athena_client.list_databases(CatalogName='AwsDataCatalog')
                  if 'sales' not in [db['Name'] for db in databases['DatabaseList']]:
                      raise Exception("Database 'sales' does not exist")

                  # Check if source table exists and has data
                  logger.info("Checking sales_records table")
                  result = run_query(athena_client, "SELECT COUNT(*) FROM sales_records", s3_output)
                  count = int(result['ResultSet']['Rows'][1]['Data'][0]['VarCharValue'])
                  if count == 0:
                      raise Exception("Table sales_records is empty")
                  logger.info(f"Table sales_records has {count} rows")

                  # Create Parquet table
                  query = f"""
                  CREATE TABLE IF NOT EXISTS sales_records_parquet
                  WITH (
                      format = 'PARQUET',
                      external_location = 's3://{s3_bucket}/athena/sales_records_parquet/'
                  )
                  AS SELECT * FROM sales_records
                  """
                  
                  logger.info(f"Starting query: {query}")
                  run_query(athena_client, query, s3_output)
                  
                  logger.info("Query completed successfully")
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              except Exception as e:
                  logger.error(f"Error: {str(e)}")
                  raise

          def run_query(athena_client, query, s3_output):
              try:
                  response = athena_client.start_query_execution(
                      QueryString=query,
                      QueryExecutionContext={'Database': 'sales'},
                      ResultConfiguration={'OutputLocation': s3_output}
                  )
                  query_execution_id = response['QueryExecutionId']
                  
                  while True:
                      query_status = athena_client.get_query_execution(QueryExecutionId=query_execution_id)
                      state = query_status['QueryExecution']['Status']['State']
                      if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                          if state != 'SUCCEEDED':
                              error_message = query_status['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
                              raise Exception(f"Query failed. Reason: {error_message}")
                          return athena_client.get_query_results(QueryExecutionId=query_execution_id)
                      time.sleep(5)
              except Exception as e:
                  logger.error(f"Error running query: {query}")
                  logger.error(f"Error details: {str(e)}")
                  raise

      Runtime: python3.8
      Timeout: 900
      Environment:
        Variables:
          OUTPUT_BUCKET: !Ref SalesDataS3Bucket

  PopulateParquetTablesCustomResource:
    Type: Custom::PopulateParquetTables
    DependsOn:
      - DownloadSalesDataCustomResource
      - SalesRecordsTable
    Properties:
      ServiceToken: !GetAtt PopulateParquetTablesLambdaFunction.Arn
      OutputBucket: !Ref SalesDataS3Bucket

  QueryAthenaLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt QueryAthenaLambdaExecutionRole.Arn
      Code:
        ZipFile: |
          import os
          import boto3
          import time
          import json

          athena_client = boto3.client('athena')
          s3_client = boto3.client('s3')

          def lambda_handler(event, context):
              try:
                  auth_context = event.get('requestContext', {}).get('authorizer', {})
                  allowed_operations = auth_context.get('allowedOperations', '')
                  
                  if 'query' != allowed_operations:  # Changed from list check to string comparison
                      return {
                          'statusCode': 403,
                          'body': json.dumps('Operation not allowed')
                      }
                  if isinstance(event.get('body'), str):
                      body = json.loads(event.get('body'))
                  else:
                      body = event.get('body')

                  if not body:
                      return {
                          'statusCode': 400,
                          'body': json.dumps('Missing request body')
                      }

                  query = body['query']
                  database = body['database']
              except KeyError as e:
                  return {
                      'statusCode': 400,
                      'body': json.dumps(f'The JSON body must contain the key: {str(e)}')
                  }
              except ValueError as e:
                  return {
                      'statusCode': 400,
                      'body': json.dumps(f'An error occurred: {str(e)}')
                  }
              except Exception as e:
                  return {
                      'statusCode': 400,
                      'body': json.dumps(f'An unexpected error occurred: {str(e)}')
                  }

              s3_output = f"s3://{os.environ['S3_OUTPUT_BUCKET']}/"

              try:
                  response = athena_client.start_query_execution(
                      QueryString=query,
                      QueryExecutionContext={
                          'Database': database
                      },
                      ResultConfiguration={
                          'OutputLocation': s3_output
                      }
                  )
                  query_execution_id = response['QueryExecutionId']
              except Exception as e:
                  return {
                      'statusCode': 400,
                      'body': json.dumps(f'Failed to start query execution: {str(e)}')
                  }

              def is_query_still_running(exec_id):
                  resp = athena_client.get_query_execution(QueryExecutionId=exec_id)
                  state = resp['QueryExecution']['Status']['State']
                  return state in ['QUEUED', 'RUNNING']

              while is_query_still_running(query_execution_id):
                  print('Query is still running...')
                  time.sleep(5)

              try:
                  final_response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)
                  if final_response['QueryExecution']['Status']['State'] == 'SUCCEEDED':
                      output_location = final_response['QueryExecution']['ResultConfiguration']['OutputLocation']
                      output_bucket, output_key = parse_s3_url(output_location)
                      result = s3_client.get_object(Bucket=output_bucket, Key=output_key)
                      result_content = result['Body'].read().decode('utf-8')
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'message': 'Query execution succeeded.',
                              'data': result_content
                          })
                      }
                  elif final_response['QueryExecution']['Status']['State'] == 'FAILED':
                      return {
                          'statusCode': 400,
                          'body': json.dumps('Query execution failed, check Athena history for the query failure reason.')
                      }
                  else:
                      return {
                          'statusCode': 400,
                          'body': json.dumps('Query execution cancelled.')
                      }
              except Exception as e:
                  return {
                      'statusCode': 400,
                      'body': json.dumps(f'Failed to get query execution status: {str(e)}')
                  }

          def parse_s3_url(url):
              if url.startswith('s3://'):
                  url = url[5:]
                  bucket, key = url.split('/', 1)
                  return bucket, key
              raise ValueError("Invalid S3 URL")
      Runtime: python3.8
      Timeout: 30
      Environment:
        Variables:
          S3_OUTPUT_BUCKET: !Ref SalesDataS3Bucket
      ReservedConcurrentExecutions: 5
  
  ApiKeysSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Name: !Sub "${AWS::StackName}-api-keys"
      GenerateSecretString:
        SecretStringTemplate: '{"clientId": "default", "allowedOperations": ["query"]}'
        GenerateStringKey: "apiKey"
        PasswordLength: 32
        ExcludeCharacters: '"@/\!$%^&*()_+<>?:{}|~`,.;[]'
  
  CustomAuthorizerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: SecretsManagerAccess
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                Resource: !Ref ApiKeysSecret

  CustomAuthorizerFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt CustomAuthorizerRole.Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import re

          class HttpVerb:
            GET = "GET"
            POST = "POST"
            PUT = "PUT"
            PATCH = "PATCH"
            HEAD = "HEAD" 
            DELETE = "DELETE"
            OPTIONS = "OPTIONS"
            ALL = "*"

          class AuthPolicy:
            awsAccountId = ""
            principalId = ""
            version = "2012-10-17"
            pathRegex = "^[/.a-zA-Z0-9-\*]+$"
            allowMethods = []
            denyMethods = []
            restApiId = "<<restApiId>>"
            region = "<<region>>"
            stage = "<<stage>>"

            def __init__(self, principal, awsAccountId):
                self.awsAccountId = awsAccountId
                self.principalId = principal
                self.allowMethods = []
                self.denyMethods = []

            def _addMethod(self, effect, verb, resource, conditions):
                if verb != "*" and not hasattr(HttpVerb, verb):
                    raise NameError("Invalid HTTP verb " + verb)
                resourcePattern = re.compile(self.pathRegex)
                if not resourcePattern.match(resource):
                    raise NameError("Invalid resource path: " + resource)

                if resource[:1] == "/":
                    resource = resource[1:]

                resourceArn = (f"arn:aws:execute-api:{self.region}:{self.awsAccountId}:"
                              f"{self.restApiId}/{self.stage}/{verb}/{resource}")

                if effect.lower() == "allow":
                    self.allowMethods.append({
                        'resourceArn': resourceArn,
                        'conditions': conditions
                    })
                elif effect.lower() == "deny":
                    self.denyMethods.append({
                        'resourceArn': resourceArn,
                        'conditions': conditions 
                    })

            def _getEmptyStatement(self, effect):
                statement = {
                    'Action': 'execute-api:Invoke',
                    'Effect': effect[:1].upper() + effect[1:].lower(),
                    'Resource': []
                }
                return statement

            def _getStatementForEffect(self, effect, methods):
                statements = []
                if len(methods) > 0:
                    statement = self._getEmptyStatement(effect)
                    for curMethod in methods:
                        if curMethod['conditions'] is None or len(curMethod['conditions']) == 0:
                            statement['Resource'].append(curMethod['resourceArn'])
                        else:
                            conditionalStatement = self._getEmptyStatement(effect)
                            conditionalStatement['Resource'].append(curMethod['resourceArn'])
                            conditionalStatement['Condition'] = curMethod['conditions']
                            statements.append(conditionalStatement)
                    statements.append(statement)
                return statements

            def allowAllMethods(self):
                self._addMethod("Allow", HttpVerb.ALL, "*", [])

            def build(self):
                if ((self.allowMethods is None or len(self.allowMethods) == 0) and
                    (self.denyMethods is None or len(self.denyMethods) == 0)):
                    raise NameError("No statements defined for the policy")

                policy = {
                    'principalId': self.principalId,
                    'policyDocument': {
                        'Version': self.version,
                        'Statement': []
                    }
                }

                policy['policyDocument']['Statement'].extend(
                    self._getStatementForEffect("Allow", self.allowMethods))
                policy['policyDocument']['Statement'].extend(
                    self._getStatementForEffect("Deny", self.denyMethods))

                return policy

          def lambda_handler(event, context):
              try:
                  print("Received event:", json.dumps(event))  # Debug log
                  
                  api_key = event.get('headers', {}).get('x-api-key')
                  if not api_key:
                      print("No API key found")
                      raise Exception('Unauthorized')
                      
                  method_arn = event.get('methodArn')
                  if not method_arn:
                      print("No methodArn found")
                      raise Exception('Missing methodArn')

                  print(f"API Key: {api_key}, Method ARN: {method_arn}")  # Debug log

                  tmp = method_arn.split(':')
                  apiGatewayArnTmp = tmp[5].split('/')
                  awsAccountId = tmp[4]

                  policy = AuthPolicy('default', awsAccountId)
                  policy.restApiId = apiGatewayArnTmp[0]
                  policy.region = tmp[3]
                  policy.stage = apiGatewayArnTmp[1]
                  
                  policy.allowAllMethods()
                  authResponse = policy.build()
                  
                  authResponse['context'] = {
                      'allowedOperations': 'query'
                  }

                  print("Response:", json.dumps(authResponse))  # Debug log
                  return authResponse

              except Exception as e:
                  print(f"Error: {str(e)}")
                  raise Exception('Unauthorized')
      Runtime: python3.8
      Timeout: 30
      Environment:
        Variables:
          SECRET_NAME: !Sub "${AWS::StackName}-api-keys"

  TextToSqlEngineAPIGateway:
    Type: AWS::ApiGateway::RestApi
    Properties:
      Name: text-to-sql-engine-APIGateway

  QueryAthenaResource:
    Type: AWS::ApiGateway::Resource
    Properties:
      ParentId: !GetAtt TextToSqlEngineAPIGateway.RootResourceId
      PathPart: query
      RestApiId: !Ref TextToSqlEngineAPIGateway

  QueryAthenaMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      HttpMethod: POST
      ResourceId: !Ref QueryAthenaResource
      RestApiId: !Ref TextToSqlEngineAPIGateway
      AuthorizationType: CUSTOM
      AuthorizerId: !Ref CustomAuthorizer
      Integration:
        Type: AWS_PROXY
        IntegrationHttpMethod: POST
        Uri: !Sub arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${QueryAthenaLambdaFunction.Arn}/invocations
      MethodResponses:
        - StatusCode: 200
        - StatusCode: 400
        - StatusCode: 401
        - StatusCode: 403

  QueryAthenaLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref QueryAthenaLambdaFunction
      Principal: apigateway.amazonaws.com
      SourceArn: !Sub arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${TextToSqlEngineAPIGateway}/*

  CustomAuthorizer:
    Type: AWS::ApiGateway::Authorizer
    Properties:
      Name: CustomApiKeyAuthorizer
      RestApiId: !Ref TextToSqlEngineAPIGateway
      Type: REQUEST
      IdentitySource: method.request.header.x-api-key
      AuthorizerUri: !Sub "arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${CustomAuthorizerFunction.Arn}/invocations"
      AuthorizerResultTtlInSeconds: 300

  CustomAuthorizerPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref CustomAuthorizerFunction
      Principal: apigateway.amazonaws.com
      SourceArn: !Sub "arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${TextToSqlEngineAPIGateway}/authorizers/${CustomAuthorizer}"


  TextToSqlEngineAPIGatewayDeployment:
    Type: AWS::ApiGateway::Deployment
    DependsOn:
      - QueryAthenaMethod
    Properties:
      RestApiId: !Ref TextToSqlEngineAPIGateway

  TextToSqlEngineAPIGatewayStage:
    Type: AWS::ApiGateway::Stage
    Properties:
      StageName: Prod
      RestApiId: !Ref TextToSqlEngineAPIGateway
      DeploymentId: !Ref TextToSqlEngineAPIGatewayDeployment
  
Outputs:
  SalesDataS3Bucket:
    Description: "S3 bucket for storing Sales datasets"
    Value: !Ref SalesDataS3Bucket

  TextToSqlEngineAPIGatewayURL:
    Description: "URL of the Unified API Gateway endpoint"
    Value: !Sub "https://${TextToSqlEngineAPIGateway}.execute-api.${AWS::Region}.amazonaws.com/Prod"
  
  ApiKeySecretArn:
    Description: "ARN of the Secret storing API keys"
    Value: !Ref ApiKeysSecret